#!/bin/bash

set -e

echo "ğŸš€ Deploying NVIDIA Device Plugin to Kubernetes..."

# Verify kubectl access
if ! kubectl get nodes >/dev/null 2>&1; then
    echo "âŒ kubectl not configured or cluster not accessible"
    echo "Make sure you're running this from a machine with kubectl access to the cluster"
    exit 1
fi

echo "âœ… kubectl access verified"

# Check if we have a GPU node
echo "ğŸ” Checking for GPU nodes..."
kubectl get nodes -o wide

# Deploy NVIDIA Device Plugin using the official manifest
echo "ğŸ“¦ Deploying NVIDIA Device Plugin..."
kubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.14.5/nvidia-device-plugin.yml

echo "â³ Waiting for device plugin to be ready..."
sleep 10

# Wait for the device plugin daemonset to be ready
kubectl rollout status daemonset/nvidia-device-plugin-daemonset -n kube-system --timeout=300s

echo "âœ… NVIDIA Device Plugin deployed successfully!"

# Verify GPU resources are available
echo "ğŸ” Checking GPU resource allocation..."
echo ""
echo "ğŸ“Š Node GPU capacity:"
kubectl describe nodes | grep -A 5 -B 5 "nvidia.com/gpu"

echo ""
echo "ğŸ“‹ GPU device plugin pods:"
kubectl get pods -n kube-system -l name=nvidia-device-plugin-ds

echo ""
echo "ğŸ¯ GPU nodes with capacity:"
kubectl get nodes -o custom-columns="NAME:.metadata.name,GPU:.status.allocatable.nvidia\.com/gpu"

echo ""
echo "âœ… GPU support is now enabled in Kubernetes!"
echo ""
echo "ğŸ“ To test GPU functionality, create a pod with:"
echo "  resources:"
echo "    limits:"
echo "      nvidia.com/gpu: 1"
echo ""
echo "ğŸ§ª Quick test command:"
echo "kubectl run gpu-test --image=nvidia/cuda:11.0-base --rm -it --restart=Never --limits='nvidia.com/gpu=1' -- nvidia-smi"